# Metadata Archiving Implementation Plan (Unified JSONB approach)

This plan replaces the previous dual-table design. All JSON/external JSON metadata is archived into a unified JSONB structure stored alongside the deleted DID record. Each numbered step is a checklist and must be completed exactly as written. Avoid inventing additional behaviours; if a backend limitation exists, mirror the intended semantics in the DAO layer and add comments referencing the affected step. Resolve any ambiguity in favour of consistency with existing Rucio patterns (SQLAlchemy models, Alembic migrations, and config parsing in `rucio.common.config`). When a decision is needed, prefer predictable defaults (deterministic ordering, explicit error paths) over implicit behaviour. If a best-practice decision is already described below, do not defer to future implementers—apply it verbatim. Where wording previously left room for interpretation, the clarifications below are mandatory and override earlier drafts.

## 1. Functional scope and defaults
- **Outcome:** When a DID is removed, all associated JSON/external JSON metadata must be persisted in a JSONB column of the deleted record; when archiving is disabled explicitly, metadata is intentionally discarded. This applies uniformly to undertaker, reaper, API-driven deletes, and any future deletion code paths. Metadata inputs include every enabled metadata plugin attached to the DID at the time of deletion (both JSON and external JSON providers), and the behaviour below is the single source of truth.
- Add `[deletion] archive_metadata = True` to `etc/rucio.cfg.template` with comments describing the default archiving behaviour and the consequences of disabling it. Default to `True` at runtime even if the config entry is missing; the flag is only valid under `[deletion]` and only accepts `True`/`False` (case-insensitive). The runtime getter must raise a `ConfigurationException` if the key is set to any other value and must not silently fall back to defaults when parsing fails. Implement a single helper (e.g., `get_archive_metadata_enabled`) in shared config code and call it from both server and client paths rather than duplicating parsing logic.
- Add `[deletion] archive_metadata_retention_days` (nullable; absence = keep indefinitely). Reject `0`/negative values during config load. Coerce string integers, reject floats, and store the resolved integer value for purge logic. Document that the value is interpreted in whole days and is applied using UTC; daylight-saving offsets must not be applied. The getter must raise a `ConfigurationException` on invalid input rather than silently clamping values. Implement this alongside the archive-enabled helper so both use identical validation.
- Define `archiving_settings` (single row `id = 1`) with `archive_enabled BOOLEAN NOT NULL`, `retention_days INTEGER NULL`, `redaction_mode TEXT NOT NULL DEFAULT 'none'`, `redaction_keys JSON/JSONB NULL` (text on backends without JSON), `redaction_salt TEXT NULL`, `updated_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT now()`, and `plugin_state JSON/JSONB NULL` (records the plugins active at deletion time). Add a check constraint `retention_days > 0` when not null and enforce single-row semantics via primary key `id = 1`. The migration must insert the row with defaults (`archive_enabled = True`, `retention_days = NULL`, `redaction_mode = 'none'`, `redaction_keys = []`, `redaction_salt = NULL`, `plugin_state = NULL`) and must be idempotent (re-run updates the row instead of inserting a duplicate). Seed data must be identical across backends; do not use dialect-specific defaults beyond JSON vs text types. When a backend lacks JSONB, store `redaction_keys`/`plugin_state` as text containing canonical JSON (sorted keys, no whitespace) and add explicit comments noting the type choice.
- Accepted `redaction_mode` values: `none`, `drop_keys`, `hash_keys`. Require `redaction_salt` when hashing; ignore it for other modes. `redaction_keys` is an array of strings applied to top-level JSON keys only. Hashing uses `HMAC-SHA256` over `json.dumps(value, sort_keys=True, separators=(',', ':'))` with the UTF-8 encoded salt. Redaction is applied only to keys present in `redaction_keys`; unmatched keys remain untouched. Empty `redaction_keys` results in no-op even if mode is `drop_keys`/`hash_keys` and must not raise. During validation, normalise `redaction_keys` to a sorted, de-duplicated list to keep storage deterministic.

## 2. Schema changes and migrations
- Extend `deleted_dids` (or the backend-specific deleted DID table) to add two JSONB-capable columns:
  - `metadata JSON/JSONB NULL`: stores the archived metadata payload as a single document. For backends without JSONB, store as deterministic JSON text (sorted keys, UTF-8). Do not split metadata across auxiliary tables. The column type must be chosen to support indexed containment operators on dialects where JSONB is available. Backends without JSONB must still enforce `NULL` vs non-`NULL` semantics identical to JSONB backends.
  - `metadata_plugins JSON/JSONB NULL`: array or object capturing the metadata plugins active when the DID was removed (e.g., plugin identifiers). This is populated at deletion time from runtime plugin configuration and should be `NULL` when archiving is disabled. Store plugin names in sorted order and avoid storing configuration secrets; only identifiers are persisted. If a plugin name is duplicated, collapse to a unique sorted set before writing. Represent the list consistently across backends (JSON array in JSONB backends; canonical JSON text elsewhere).
- Ensure `archived_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT now()` exists on the deleted DID table (add it if missing). Add indices on `(scope, name)` and `(archived_at)` for retrieval and retention operations. Do not add unique constraints tied to metadata content. Prefer naming indices with `ix_<table>_<columns>` generated through `op.f`. Explicitly mark indices as non-unique and include `postgresql_using='btree'` where applicable to avoid dialect defaults. On SQLite/MySQL, use the dialect-supported index syntax and avoid partial indices.
- Update SQLAlchemy models to reflect the new columns. Keep model definitions collocated with existing DID/deleted DID models. Ensure Alembic autogeneration includes the new columns; if autogeneration misses dialect-specific defaults, hand-edit the migration to match the schema definitions precisely. Model attributes must align with the JSON/text choice per dialect to avoid runtime deserialization errors. Document any manual edits directly in the migration comments so future autogenerate comparisons stay understandable.

## 3. Archiving behaviour and trigger functions
- Implement backend-specific trigger functions (or DAO hooks where triggers are unavailable) that, on deletion of a DID, copy the deleted row into the deleted table while populating `metadata` and `metadata_plugins`:
  - For JSON metadata plugins, serialize the metadata payload into JSONB and assign to `metadata`. Use consistent serialization (sorted keys, UTF-8) to avoid nondeterministic diffs between backends. If a plugin returns `None`, omit the key entirely rather than storing `null`. Treat empty dictionaries as valid payloads and store them without stripping.
  - For external JSON plugins, implement a conversion to JSONB/text before insert. Fail the deletion transaction if conversion fails to avoid silent data loss. Explicitly log the plugin name and DID causing the failure. Conversion must not mutate the original payload; copy before transformation. Do not silently drop fields that fail to convert—fail the transaction instead.
  - Populate `metadata_plugins` with a deterministic representation of active plugins (e.g., a sorted array of plugin names). Source this from the runtime plugin configuration so it reflects actual behaviour at deletion time. The list must include both JSON and external JSON providers in one combined sorted set. Persist exactly the plugin identifiers used in configuration (no aliasing or user-friendly renames).
- Trigger behaviour must read `archiving_settings.archive_enabled`; if False, skip populating metadata fields (store null) but still insert the deleted DID record. If the settings row cannot be read, fail the delete. Settings read must be performed within the same transaction as the delete to avoid races. Do not cache settings across transactions; always re-read to honour live changes. If the settings row is missing entirely, create a blocking failure with a clear log message rather than inserting default values.
- Apply redaction inside the trigger/DAO before writing `metadata`, using the settings row (`redaction_mode`, `redaction_keys`, `redaction_salt`). Redaction only alters the archived `metadata` payload; the live table row being deleted should remain untouched before the copy. For `drop_keys`, remove the key entirely; for `hash_keys`, replace the value with the hex digest; for `none`, copy the payload verbatim. Redaction must occur after the metadata payload is fully assembled from plugins but before serialization to text/JSONB. If redaction settings are invalid (e.g., `hash_keys` without a salt), abort the transaction with a descriptive error.
- Trigger names should follow `trg_archive_deleted_did_after_delete` (or backend equivalent). Drop triggers/functions in downgrades before removing columns if necessary. Where triggers are unsupported, implement the same logic in the DAO delete call and ensure transactions match the trigger semantics (single atomic delete+insert+redaction). DAO hooks must be unit-tested separately to guarantee parity with trigger behaviour.

## 4. Application workflow integration
- Ensure undertaker and any deletion APIs funnel through the DAO path that invokes the trigger or equivalent hook so metadata archiving always executes. Remove any fast paths that bypass triggers for tests. All code paths that delete DIDs must be covered by automated tests validating that the hooks fire. Explicitly forbid direct `DELETE` statements in code paths covered by this plan; only DAO methods may perform deletes. If existing code uses raw SQL deletes, refactor them as part of this work.
- Reaper replica-finalization must rely on the same deletion DAO. Avoid duplicate archiving logic in Python; the trigger/DAO copy is the single source of truth. When refactoring, ensure there is a single call site responsible for deletes to prevent drift. Document the canonical DAO entry point in module-level comments to guide future contributors.
- Update code comments to state that archiving of metadata is handled at the database boundary and must not be reimplemented elsewhere. Comments should reference this plan section to discourage future duplication. Where legacy helper functions exist, annotate them to indicate they must not store metadata independently.

## 5. Tests
- Add unit tests covering:
  - Archiving enabled: `metadata` and `metadata_plugins` populated with exact payload and plugin list. Include ordering checks to ensure deterministic serialization. Assert the stored plugin list equals the sorted unique configuration names.
  - Archiving disabled: `metadata`/`metadata_plugins` are null while deleted row is present. Validate that the `archiving_settings` row remains unchanged when archiving is disabled.
  - Redaction: keys are dropped/hashed per settings; hashing is deterministic given the salt. Verify that missing keys are untouched and empty `redaction_keys` is a no-op. Include both JSONB and text backends to ensure hash/drop logic is backend-agnostic.
  - Failure cases: JSONB conversion failure for external plugins rolls back the delete. Assert the original DID remains undeleted when the transaction fails. Also cover missing settings row and invalid redaction configuration as hard failures.
- Add integration tests for undertaker and reaper paths across JSON and external JSON plugins, asserting stored `metadata` matches source payloads (order-insensitive) and `metadata_plugins` matches configured plugins. Include a test that verifies `archived_at` is non-null and retention queries operate on it. Cover both DBs with and without native JSON support. Ensure tests perform deletes through the DAO layer rather than ad-hoc SQL. Include a retention test that runs purge after a controlled time offset to verify UTC cutoffs.

## 6. Retention and purge
- Purge logic reads `archiving_settings.retention_days`; if null, log “retention disabled” and exit. Otherwise, delete rows from the deleted DID table where `archived_at` is older than the cutoff. Index on `archived_at` should be used for ordering. Compute cutoffs using UTC-aware timestamps and do not reuse stale configuration between batches. Purge must honour `archive_enabled` only for logging; retention is independent of whether archiving is currently on or off.
- Provide CLI options `--batch-size`, `--dry-run`, and `--max-duration` as in prior plan. Default batch size = 500; reject non-positive values. Order deletes by `archived_at ASC` for determinism. CLI defaults must match server defaults to avoid operator confusion; document them in `--help` text. When both `--max-duration` and `--batch-size` are set, stop processing when either limit is reached without partial batches being silently dropped.
- Log counts examined/deleted/skipped and exit codes. Integration tests should cover retention honoured, dry-run, batching, `max-duration`, and ordering by `archived_at`. Tests must also assert that dry-run performs no writes (including no metric increments for deleted rows). Add a test that ensures purge exits cleanly when the settings row is missing, with a clear error message and non-zero status.

## 7. Observability
- On startup of undertaker/reaper, log whether archiving is enabled, the retention window, and active plugins. Include metrics counters for archived rows, failures, purge deletions, and purge failures, labeled by backend. Add a gauge for the age of the last `archiving_settings.updated_at`. Emit these logs after configuration is fully resolved so values match runtime behaviour.
- Provide a helper (CLI/SQL) to retrieve archived metadata for `(scope, name)`, returning `metadata`, `metadata_plugins`, and `archived_at` ordered by `archived_at DESC`. Default limit 100. Explicitly redact according to stored payload only—do not reapply runtime redaction settings during retrieval.

## 8. Documentation and rollout
- Update user/admin docs to describe the unified JSONB archiving approach, the role of `metadata_plugins`, how to enable/disable archiving, and how retention/purge operate. Include warnings about data loss when archiving is disabled. Document backend-specific storage types (JSONB vs text) so operators know what to expect in backups.
- Release notes must call out the schema changes (new columns on deleted table, settings table update), the default-on archiving behaviour, and any need to backfill historical deletes if desired. Mention that redaction applies only to archived copies, not to live metadata.
- Provide a staged deployment checklist: apply migrations, deploy code with archiving off, enable archiving, verify deleted rows show `metadata` and `metadata_plugins`, and confirm triggers/settings exist. Include rollback guidance: disable archiving, drain workers, and leave archived data intact unless explicitly purged. Add an explicit step to verify that the `archiving_settings` row exists and matches defaults after migration on every backend before enabling workers.

## 9. Ownership and maintenance
- Assign maintainers for the deleted DID schema and purge tooling in developer docs. Add CI guardrails ensuring new deletion entry points include archiving coverage and that migrations accompany schema changes touching deleted DIDs. Schedule periodic review of archive size and performance, capturing results in an operational log.
